INTRO TO DOCKER

Docker Hub has thousands of pre-made containers that you can download instead
of building your own.

From you regular environment, run:

docker pull mongo:3

to get the container off of docker hub.

Run:

docker ps to show which docker containers are currently runing

------------------------------------------------------------------------------------------------------------------------------------

DOCKER IMAGES WITHOUT DOCKER DEMO

 A pre-made container is called and image and you can layer them. An image captures the state of a container
 and tars it together in essentially a zip file.

 I want to run the container within a container just to mimic BH setup and avoid
 issues that i was starting to encounter in the previous section of the tutorial.

 NOTE: that you can remove containers by running:

 docker container ls -a

 grab the container ID then run:

 docker container rm [with the CID here]


1). start a docker container with docker running in it connected to host docker daemon

docker run -ti -v /var/run/docker.sock:/var/run/docker.sock --privileged --rm --name docker-host docker:18.06.1-ce

This is going to run a docker container that has a docker client that is connected to you host computer (or VM).
It will connect to the docker that's running on docker desktop. I'm not running docker desktop though but
I believe that the docker daemon (dockerd) is running in the background on linux.

I'm not completely sure what this command is doing, but typically you cannot connect outside of the host but
the -v flag allows you to do so (opens a tunnel to the host docker container). It also downloaded docker:18.06.1-ce
in order to execute the command.


2). upon running cat/etc/issue from within the docker container we just downloaded and are running we see that
we are running a Linux Alpine container

3). if we docker ps from within our container we can see outside of the container and see that docker:18.06.1-ce
is running (i believe we named it docker-host). Again this is atypical

4). Now from within our Alpine container we want to chroot, unshare, and cgroup another container that we download
from dockerhub so that we can show how containers (mostly) work manually. The first step we're going to take is to
download and run a new container that does nothing. We will basically then copy this container's state (whic is what
an image is) and run it manually (chroot, unshare, cgroup) without docker:

docker run --rm -dit --name my-alpine alpine:3.10 sh

(this will run ANOTHER alpine container that we are going to copy)

5). Run docker ps to see that both the my-alpine and the docker-host containers are running (on the bare metal).

6). To copy the state of the my-alpine container and create and image run:

docker export -o dockercontainer.tar my-alpine

7). ls to see the .tar image file

8). create the directory into which we are going to "unzip" the my-apine image using:

mkdir container-root

9). unzip the image and place the contents into our newly created manual container root directory:

tar xf dockercontainer.tar -C container-root

10). ls into container root to see that it is a familiar looking linux filesystem

11). Set up a cgroup:

----------------------------------------------------------------------------------------------------------------------
ALPINE LINUX CGROUP SETUP:

It looks like BH skipped this piece but I want to try it. Unfortunately, it doesn't look like Linux Alpine has
a cgroup-tools container, so I may have to try setting up cgroups manually which seems like an interesting challenge.

You can use the alpine package manager to install htop though:

apk add htop

Note that when running htop I'm seeing all 16GB of my memory available which is surprising. After
all I'm already one level deep inside an Alpine container.

Running cgroups manually (https://www.redhat.com/sysadmin/cgroups-part-three):

- By default the cgroup directory structure will be created in /sys/fs/cgroup (confirmed)
- You could recreate the folder structure by hand but in our case Alpine does have the struture already:
/my_cgroups
├── <controller type>
│   ├── <group 1>
│   ├── <group 2>
│   ├── <group 3>

- you would then mount the cgroups into those folders (mount -t cgroup -o memory none /my_cgroups/memory)
- you could create your own cgroups (mkdir -p /my_cgroups/cpu/{user1,user2,user3})
- The directories are then automatically populated by the controller
- Tutorial uses cat /dev/urandom to create a load on the cpu
- You can change cpu.shares to adjust priority for processes (echo 2048 > user1/cpu.shares)
- To add a process to a cgroup add the desired PID to the tasks file:
echo 2023 > user1/tasks
- It actually looks like in this case, cpu shares are determined by top-level cgroup priority. So if user1 has
priority 2048, user2 has priority 768 and user3 has priority 512 user one will get:

2048/(768+512+2048) = 61.5% of CPU time:

I'm going to try to set this up in alpine linux (NOTE: this should be done in the top Alpine container, right below bare metal):

1). cd into /sys/fs/cgroup

2). create a cgroup we want to use for the manual docker container we'll run:

mkdir -p cpu/manual_docker

3). ls into cpu/manual docker to confirm that it is automatically populated by the controller

4). create a priority for the cgroup:

echo 256 > cpu/manual_docker/cpu.shares

5). I'm not entirely sure what I'm doing, but in the BH video he runs the following command:

cgset -r cpu.cfs_period_us=100000 -r cpu.cfs_quota_us=$[ 5000 * $(getconf _NPROCESSORS_ONLN) ] sandbox

and in our manual_docker cgroup there is a cpu.cfs_period_us and a cpu.cfs_quota_us
the period is alread set to 100000 and quota is set to -1. I'm going to try to set it to 5000.
Not sure how all this affects the cpu share in the end

TODO: Look into depth how to set up the cpu share properly

run:

echo 5000 > manual_docker/cpu.cfs_quota_us


NOTE: I'm not sure how to properly set up the memory cgroup as well. I was under the impression that you
could use one cgroup to set the cpu and the memory at the same time, but with the file structure in place
it seems a little uncertain (could try to create a new manual_docker folder under the /sys/fs/cgroup/memory
directory. It's a container so it seems like a pretty safe thing to try).
------------------------------------------------------------------------------------------------------------------------------------


12). Now recap the situation so that you don't do anything stupid because it's feeling like inception at the moment:

    - You have ubuntu on your actual computer
    - You are running an unusual docker container (docker-host) that tunnels out to connect to your actual computer so that
    when you run docker ps from within that container it actually sees itself (this is my current limited understanding)
    - You are running an Alpine Linux container from within the docker-host container. It is doing nothing at the moment but
    you used it to extract its state/linux directory setup and you are now ready to run a new version of it manually.
    - in order to run it manually you set up a cgroup for it manually and the next step is to run the container manually
    using chroot and unshare.
    - you will then want to start another shell in the docker-host container to view its PID and assign the docker-manual cgroup
    to it.
    - if you manage to do all that you deserve a pat on the back

    a). So at this point we are ready to run the manual docker image with:

    unshare --mount --uts --ipc --net --pid --fok --user --map-root-user chroot /container-root sh

    b). Next start a new shell in the docker-host alpine container:

    sudo docker exec -it docker-host /bin/sh

    Note: that in alpine the shell isn't bash it's ash

    In that shell we can docker ps and grab the container ID of the container that we are trying to manually run (my-alpine)

    c). Next we're going to add the sh (ash) process we just created to the manual_docker cgroup we set up:

    echo 135 > manual_docker/tasks

    d). within our manual container, let's try running a command that would normally pin the cpu:

    yes > /dev/null

    Confirmed success! It pinned the cpu available in the manual container but from the outside it was only using 5% cpu!

    e). To try out limiting the RAM usage, I created a new manual_docker directory in /sys/fs/cgroup/memory. Next I
    added ran the following command to limit the memory usage to 80M:

    echo 83886080 > memory.limit_in_bytes

    and added the process ID of the manually run container to the memory cgroup like so:

    echo 136 > tasks

    f). In order to see if it worked, pin the RAM:

    yes | tr \\n x | head -c 104857600 | grep n

    confirmed that the ram was not pinned. It's hard to tell if it really worked because 8M/16G is 0.5%
    I'm going to increase the ram allowance so that we end up with 2% being used by the process (0.02 * 16000000000 = 32000000)
    I think the actual value in bytes is 335555320

    It didn't seem to work. Even after restarting the manual container, the RAM pinning command actually stops running very
    quickly. It runs for a quick second and immediately shuts down. Not sure what's going on, but it's possible that it
    is not being limited by the cgroup and some automatic process comes to shut it down before it crashes everything? Seems
    a little unlikely. Not sure what's happening here.

    TODO: Follow up
----------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------
TODO: Figure out how to properly start and stop the docker daemon.
It looks like you can run:

sudo docker info

or

sudo systemctl is-active docker

to confirm whether or not the docker daemon is running. You can also check the process list for dockerd.

Apparently on a typical install, the docker daemon (dockerd) is started by a system utility and not manually
by a user. This makes automatically restarting docker on reboot easier. That being said, I'm not sure I want
docker running in the background at all times for no reason.

To manually start the docker daemon you can simply run "dockerd" in the forground.


To configure the docker daemon you can either use flags when alling dockerd manually or
modify the JSON config file. Don't overlap flags and config file.

For the JSON file you can create it at

/etc/docker/daemon.json

to view docker data more generally go to

/var/lib/docker

To enable/disable dockerd from running on boot run the following commands:

sudo systemctl enable docker.service
sudo systemctl enable conatinerd.service

or


sudo systemctl disable docker.service
sudo systemctl disable conatinerd.service

To see whether or not docker.service is enabled or disabled, i believe that you can run this command:


sudo systemctl list-unit-files --state=disabled | grep docker
-----------------------------------------------------------------------------------------------------------------------------


RUNNING DOCKER IMAGES WITH DOCKER

The docker run command is the most useful command for developers.

DOCKER BASICS

1). Run the latest alpine container:

docker run --interactive --tty run alpine

(automatically runs alpine:latest)



2). To view what you are running:

cat /etc/issue


Remember that containers are ephemeral and if you don't back things up they will be gone when you quit the container.

3). docker run -it alpine:3.10
will put you into the interactive shell. If you don't use the -it flag it will run the container and immediately close
That being said you could run a specific command inside a container and then close when the command finishes execcution:

docker run alpine:3.10 ls

This will spin up a container, run ls, print to your terminal and exit the container. NOTE: that order matter or this
command


4). docker run ubuntu:bionic cat /etc/issue
This will show that the container runs ubuntu bionic and then shuts down.


5). It's very easy to have too many images on your computer so run:

docker image prune

to free up space and delete your images (you'll lose everything you did with those containers)


6). Spits out the hash of the running container and runs the container in the background (as opposed to foreground like before)

docker run -it --detach ubuntu:bionic


7). Show what containers are currently running with

docker ps

----------------------------------------------------------------------------------------------------------------------

NODE WITH DOCKER:

1). run the node REPL (Read, Evaluate, Print, Loop which is the language shell you enter in when you
run node interactively) from the official Node linux which was created for Debian:

docker run -it node:12-stretch

12-stretch means Node 12 and Debian Stretch (which is Debian 9 or something of that nature)

since we didn't specify a specific command that we wanted Node to run, but we also didn't just run an interactive
bash shell, when we exit the Node REPL, it will exit out of the container.

You can drop into the bash container with:

docker run -it node:12-stretch bash

From there you can cat /etc/issue or run Node (that way when you exit the Node REPL you don't immediately
exit and lose the container)

NOTE: If you wanted to check what version Linux your Node container is running on you can run:
docker run -it node:12-stretch cat /etc/issue

----------------------------------------------------------------------------------------------------------------------

TAGS AND CHOOSING CONTAINER VERSIONS

When you're deciding what container version to use, you can look at its docker hub page. So fo example,
if you're looking or a Node container, you an go to https://hub.docker.com/_/node and you will see a bunch of
entries. Each row contains tags that point to the same container even though they have different names.
There's no real strict syntax/format for these tags.

When you're trying to choose one, a good rule of thumb is to choose the latest supported version (LTS). For a Node
container you would use the LTS of Node and the LTS of docker. That being said, given that you want to freeze you
setup in time so that it doesn't break when a new LTS is released, it's better not to use the lts tag.

So instead of running "docker run -it node:lts", you would run "docker run -it node:12-stretch" or whatever
the latest stable version of node and debian are. If you're just messing around you can use the experimental
latest version of the container (tagged as "latest"), but don't do this if you're shipping something.

NOTE: If you look at the following tag "12.13.1-stretch" it means use the latest stretch release of Debian (there
could be newer versions of Debian), and 12 is the node major version, 13 is the minor version, and 1 is the patch. If you use the 12.13-stretch tag, the patches will upload automatically, and if you use the 12-stretch tag you'll get updates for the major version and patches automatically. If you trust the container creater usually it's safe to just specify the major version.

You also have container versions that try to only package the bare bones (slim versions), and some very specialized
ones like chakracore (the deprecated  microsoft edge equivalent to the chromium v8 engine).

----------------------------------------------------------------------------------------------------------------------
Docker CLI

1). Pull a container from docker hub and store/cache it on your computer (in this case we are asking for the
implicitly latest version of a container jturpin/hollywood that can run the command "hollywood" that
does all kinds of impressive looking but useless things on your computer. It's a gag that simulates
the kind of things a hacker would do in a movie)

docker pull jturpin/hollywood

2). Run a command via an interactive container (-it means it won't quit immediately, and it's running
the hollywood command in the jturpin/hollywood container)

docker run -it jturpin/hollywood hollywood

3). Output useful information regarding a specific container (prints out info like the container's hash,
tags, the command it runs, environmental variables, the entrypoint, etc.)

docker inspect node:12-stretch

4). Run container (and command) in detached mode (running in background):

docker run -dit jturpin/hollywood hollywood

5). View info on containers/docker processes that are currently up:

docker ps

6). Pause a container (freezing all process trees):

docker pause [CONTAINER ID GOES HERE]

7). Unpause a container:

docker unpause [CONTAINER ID GOES HERE]

8). Kill a container:

docker kill [CONTAINER ID GOES HERE]

9). List out all the docker processes/containers but only their IDs

docker ps -q

10). Kill all containers at once:

docker kill $(docker ps -q)

11). Run a command/process (in this case "ps aux") in an existing container
(docker run starts a new container):

docker exec [CONTAINER ID OR CONTAINER NAME] ps aux

NOTE: to go back into the pre-existing container's shell you could execute "docker exec -it [CID/NAME] bash"

12). View all the changes in the history of a container:

docker history node:12-stretch

13). Get information about the container you are running. If you are ssh'd into a VM running in the cloud
this is useful to get information about the host computer you are on:

docker info

14). See all process that are going on in a container:

docker top [CID/HASH/NAME]

15). View all containers on your machine:

docker ps --all

16). View logs for a container

docker logs [CID/NAME/HASH]

17). Delete container (not the image):

docker rm [CID/NAME/HASH]

18). Delete image:

docker rmi [CID/NAME/HASH]

19). Remove all stopped containers (not images):

docker container prune

20). Remove all (non-running?) images:

docker image prune

21). List all images:

docker image list

22). Restart a docker image. It sends a Restart/Terminate signal. If that signal isn't accepted after a few
seconds (10?) docker forces a restart:

docker restart [CID/HASH?NAME]

23). Search docker hub for containers (in this case python containers):

docker search python

----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------
